# âœ¨ Scrapey CLI

[![Build & Test](https://github.com/heinrichb/scrapey-cli/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/heinrichb/scrapey-cli/actions/workflows/ci.yml)
[![Go Reference](https://pkg.go.dev/badge/github.com/heinrichb/scrapey-cli.svg)](https://pkg.go.dev/github.com/heinrichb/scrapey-cli)
[![Coverage Status](https://img.shields.io/badge/coverage-0%25-red)](https://example.com/coverage)

Scrapey CLI is a lightweight, configurable web crawler and scraper. It collects data from websites based on rules defined in a config file. It can handle HTML parsing, data extraction, and plans to offer multiple storage options (JSON, XML, Excel, databases, etc.).

---

## ğŸš€ Features

- Lightweight and modular CLI interface
- Configurable input (`.json` config file or command-line flags)
- Extensible parsing logic for targeted HTML elements
- Future support for multiple storage options (JSON, XML, Excel, MongoDB, MySQL)
- DRY and clean code principles

---

## ğŸŒ± Getting Started

1. **Clone the repo**:
   git clone https://github.com/heinrichb/scrapey-cli.git

2. **Initialize Go modules**:
   cd scrapey-cli
   go mod tidy

3. **Build the CLI**:
   go build ./cmd/scrapeycli

4. **Run**:
   ./scrapeycli --config configs/default.json

---

## âš™ï¸ Project Structure

```
scrapey-cli/
â”œâ”€â”€ cmd/
â”‚ â””â”€â”€ scrapeycli/
â”‚ â””â”€â”€ main.go # CLI entry point
â”œâ”€â”€ pkg/
â”‚ â”œâ”€â”€ config/ # Config loading logic
â”‚ â”œâ”€â”€ crawler/ # Core web crawling logic
â”‚ â”œâ”€â”€ parser/ # HTML parsing logic
â”‚ â””â”€â”€ storage/ # JSON/other storage logic
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ default.json # Example config
â”œâ”€â”€ .github/
â”‚ â””â”€â”€ workflows/
â”‚ â””â”€â”€ ci.yml # CI/CD pipeline config
â”œâ”€â”€ docs/ # Additional documentation
â”œâ”€â”€ build/ # Build scripts, Dockerfiles, etc.
â”œâ”€â”€ test/ # Optional integration tests
â””â”€â”€ README.md # This file
```

---

## ğŸ›  Usage

- **Basic**:
  ./scrapeycli --url https://example.com

- **With config file**:
  ./scrapeycli --config configs/default.json

- **Future**:
  - Save data to JSON
  - Multiple URLs at once
  - Concurrency and rate-limiting

---

## ğŸ§ª Tests

- Run unit tests locally:
  go test ./...

- Automated tests on GitHub Actions:
  - Triggered on every push and pull request to main or develop branches.
  - See Build & Test (https://github.com/heinrichb/scrapey-cli/actions) for logs and results.

---

## ğŸ¤ Contributing

1. Fork the project
2. Create your feature branch (git checkout -b feature/amazing-feature)
3. Commit your changes (git commit -m 'Add some amazing feature')
4. Push to the branch (git push origin feature/amazing-feature)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License (LICENSE).

---

Happy Scraping!
